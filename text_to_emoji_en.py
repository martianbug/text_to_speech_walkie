from typing import List, Tuple, Dict
import spacy
from collections import Counter
import emoji
import re

nlp = spacy.load("en_core_web_sm")
EMOJI_MAP: Dict[str, str] = {
    # --- Feelings / people ---
    "love": "â¤ï¸", "happy": "ðŸ˜„", "joy": "ðŸ˜„", "sad": "ðŸ˜¢", "cry": "ðŸ˜¢",
    "laugh": "ðŸ˜‚", "surprise": "ðŸ˜²", "angry": "ðŸ˜ ", "party": "ðŸŽ‰",
    "celebrate": "ðŸŽ‰", "celebration": "ðŸŽ‰",

    # --- Festivals & Holidays ---
    "festival": "ðŸŽŠ", "concert": "ðŸŽ¤", "music": "ðŸŽµ", "band": "ðŸŽ¸",
    "dj": "ðŸŽ§", "dance": "ðŸ’ƒ", "parade": "ðŸ¥",
    "fireworks": "ðŸŽ†", "lantern": "ðŸ®",
    "halloween": "ðŸŽƒ", "pumpkin": "ðŸŽƒ", "ghost": "ðŸ‘»",
    "christmas": "ðŸŽ„", "xmas": "ðŸŽ„", "santa": "ðŸŽ…",
    "easter": "ðŸ°", "egg": "ðŸ¥š",
    "thanksgiving": "ðŸ¦ƒ", "turkey": "ðŸ¦ƒ",
    "new year": "ðŸŽ†", "countdown": "â³",
    "valentine": "ðŸ’˜",
    "wedding": "ðŸ’’", "marriage": "ðŸ’",
    "birthday": "ðŸŽ‚", "anniversary": "ðŸ’ž",
    "carnival": "ðŸŽ­", "masquerade": "ðŸŽ­",
    "oktoberfest": "ðŸº", "beer": "ðŸº",
    "diwali": "ðŸª”",
    "hanukkah": "ðŸ•Ž",
    "ramadan": "ðŸ•Œ", "eid": "ðŸ•Œ",
    "festival lights": "ðŸª”",
    "independence": "ðŸŽ†",

    # --- Technology / Digital life ---
    "computer": "ðŸ’»", "laptop": "ðŸ’»", "desktop": "ðŸ–¥ï¸",
    "smartphone": "ðŸ“±", "phone": "ðŸ“±", "tablet": "ðŸ“±",
    "camera": "ðŸ“·", "photo": "ðŸ“¸", "selfie": "ðŸ¤³",
    "video": "ðŸŽ¥", "stream": "ðŸ“º", "livestream": "ðŸ“º",
    "game": "ðŸŽ®", "gamer": "ðŸŽ®", "gaming": "ðŸŽ®",
    "virtual reality": "ðŸ•¶ï¸", "vr": "ðŸ•¶ï¸",
    "augmented reality": "ðŸ•¶ï¸",
    "robot": "ðŸ¤–", "ai": "ðŸ¤–",
    "code": "ðŸ’»", "programming": "ðŸ’»", "developer": "ðŸ‘©â€ðŸ’»",
    "keyboard": "âŒ¨ï¸", "mouse": "ðŸ–±ï¸",
    "server": "ðŸ–¥ï¸", "cloud": "â˜ï¸",
    "internet": "ðŸŒ", "web": "ðŸŒ", "website": "ðŸŒ",
    "wifi": "ðŸ“¶", "network": "ðŸ“¡",
    "email": "âœ‰ï¸", "message": "âœ‰ï¸", "chat": "ðŸ’¬",
    "call": "ðŸ“ž", "video call": "ðŸ“¹",
    "social": "ðŸ’¬", "social media": "ðŸ’¬",
    "streaming": "ðŸŽ¬", "music app": "ðŸŽµ",
    "crypto": "â‚¿", "bitcoin": "â‚¿", "blockchain": "ðŸ”—",
    "database": "ðŸ—„ï¸",
    "printer": "ðŸ–¨ï¸",
    "headphones": "ðŸŽ§", "earbuds": "ðŸŽ§",
    "drone": "ðŸš",
    "camera drone": "ðŸš",
    "watch": "âŒš", "smartwatch": "âŒš",
    "microphone": "ðŸŽ™ï¸",
    "usb": "ðŸ”Œ", "charger": "ðŸ”Œ",
    "energy": "âš¡", "battery": "ðŸ”‹",

    # --- Other general categories for completeness ---
    "travel": "ðŸŒ", "plane": "âœˆï¸", "airport": "ðŸ›«",
    "train": "ðŸš†", "car": "ðŸš—", "bus": "ðŸšŒ",
    "food": "ðŸ½ï¸", "pizza": "ðŸ•", "burger": "ðŸ”",
    "coffee": "â˜•", "tea": "ðŸµ",
    "sun": "â˜€ï¸", "rain": "ðŸŒ§ï¸", "snow": "â„ï¸",
    "work": "ðŸ’¼", "study": "ðŸ“š",
    "money": "ðŸ’°", "pay": "ðŸ’³",
}

def normalize_token(tok: str) -> str:
    tok = tok.lower().strip()
    tok = re.sub(r"[^\w']+", "", tok)  # keep apostrophes for contractions
    return tok

def extract_keywords(text: str, topn: int = 5) -> List[Tuple[str, float]]:
    """
    Extract candidate keywords using spaCy (EN).
    Consider NOUN, PROPN, VERB, ADJ. Use lemmas and noun chunks.
    Returns list of (lemma, score) ordered by score desc.
    """
    doc = nlp(text)
    candidates = []

    for token in doc:
        if token.is_stop or token.is_punct or token.like_num:
            continue
        if token.pos_ in {"NOUN", "PROPN", "VERB", "ADJ"}:
            lemma = normalize_token(token.lemma_)
            if lemma:
                candidates.append(lemma)

    # noun chunks (boost)
    for nc in doc.noun_chunks:
        lemma = normalize_token(nc.root.lemma_)
        if lemma:
            candidates.extend([lemma] * 2)

    counts = Counter(candidates)
    if not counts:
        return []

    most = counts.most_common(topn)
    max_cnt = most[0][1]
    return [(kw, cnt / max_cnt) for kw, cnt in most]

def map_keywords_to_emojis(keywords: List[Tuple[str, float]], max_emojis: int = 5) -> List[str]:
    chosen = []
    seen = set()
    for lemma, score in keywords:
        if lemma in EMOJI_MAP:
            e = EMOJI_MAP[lemma]
            if e not in seen:
                chosen.append(e); seen.add(e)
            if len(chosen) >= max_emojis: break
            continue
        # substring match
        found = False
        for key, em in EMOJI_MAP.items():
            if lemma in key or key in lemma:
                if em not in seen:
                    chosen.append(em); seen.add(em)
                found = True
                break
        if found and len(chosen) >= max_emojis: break
    return chosen

def text_to_emoji_summary(text: str, max_keywords: int = 6, max_emojis: int = 5) -> str:
    kws = extract_keywords(text, topn=max_keywords)
    emojis = map_keywords_to_emojis(kws, max_emojis=max_emojis)
    if emojis:
        return " ".join(emojis)
    return " / ".join([kw for kw, _ in kws]) or ""

if __name__ == "__main__":
    examples = [
        "I went to the beach with my dog, we ate pizza and drank beer. It was a perfect day.",
        "I have a work meeting at 10 AM, I need to prepare the presentation and bring my laptop.",
        "I'm very tired, I want to sleep and dream about traveling."
    ]
    for t in examples:
        print("Text:", t)
        print("Emojis:", text_to_emoji_summary(t))
        print("---")
